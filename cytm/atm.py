import logging
import numpy as np
import pandas as pd
import time

from . import atm_c as atm
from .util import perplexity

from gensim import matutils
from pathlib import Path

from tqdm import tqdm, tqdm_notebook

notebook = False
from_streamlit = False


def train(input_csv,
          k,
          alpha,
          beta,
          top_words=20,
          n_iter=1000,
          report_every=100,
          prefix="atm",
          output_dir=".",
          verbose=False):

    logging.info("Reading topic modeling corpus: {:s}".format(input_csv))

    D, A, vocab, authors, word2id, author2id = read_corpus(input_csv)
    L = sum(len(d) for d in D)
    K = k
    N = len(D)
    V = len(vocab)
    S = len(authors)

    Z = assign_random_topic(D, K)
    X = assign_random_author(D, A)

    logging.info("Corpus size: {:d} docs, {:d} words".format(N, L))
    logging.info("Vocabuary size: {:d}".format(V))
    logging.info("Number of topics: {:d}".format(K))
    logging.info("Number of authors: {:d}".format(S))
    logging.info("alpha: {:.3f}".format(alpha))
    logging.info("beta: {:.3f}".format(beta))

    n_kw = np.zeros((K, V), dtype=np.int32)  # number of word w assigned to topic k
    n_dk = np.zeros((N, K), dtype=np.int32)  # number of words in document d assigned to topic k
    n_ak = np.zeros((S, K), dtype=np.int32)  # number of topic k generated by author y
    n_k = np.zeros((K), dtype=np.int32)  # total number of words assigned to topic k
    n_d = np.zeros((N), dtype=np.int32)  # number of word in document (document length)
    n_a = np.zeros((S), dtype=np.int32)  # total number of topics generated by author y
    n_ad = np.zeros((N), dtype=np.int32) # number of author in document (author length)

    atm.init(D, A, Z, X, n_kw, n_dk, n_ak, n_k, n_d, n_a, n_ad)

    logging.info("Running Gibbs sampling inference: ")
    logging.info("Number of sampling iterations: {:d}".format(n_iter))

    start = time.time()

    pbar, pbar_text = get_progress_bar(n_iter)

    ppl = 0.0
    for i in range(n_iter):
        atm.inference(D, A, Z, X, L, n_kw, n_dk, n_ak, n_k, n_d, n_a, n_ad, alpha, beta)

        if i % report_every == 0:
            ppl = perplexity(L, n_kw, n_k, n_dk, n_d, alpha, beta)

        update_progress(i, n_iter, pbar, pbar_text, ppl)

    elapsed = time.time() - start

    ppl = perplexity(L, n_kw, n_k, n_dk, n_d, alpha, beta)
    logging.info("Sampling completed! Elapsed {:.4f} sec ppl={:.3f}".format(
        elapsed, ppl))

    save(vocab, authors, Z, n_kw, n_dk, n_ak, n_k, n_d, n_a, alpha, beta, prefix=prefix, output_dir=output_dir)


def update_progress(i, n_iter, pbar, st_text, ppl):
    if hasattr(pbar, 'progress'):
        pbar.progress(100*(i+1)//n_iter)
        st_text.text(f'Iteration {i+1} ppl={ppl}')
    else:
        pbar.update(n=1)
        pbar.set_postfix(ppl="{:.3f}".format(ppl))


def read_corpus(corpus):
    df = pd.read_csv(corpus, header=None)
    df.fillna("", inplace=True)

    D, A, vocab, authors = [], [], [], []
    word2id, author2id = {}, {}

    if notebook:
        pbar = tqdm_notebook(total=len(df))
    else:
        pbar = tqdm(total=len(df))

    for row in df.iterrows():
        values = row[1].values
        id_doc, id_author = [], []
        for w in values[0].split():
            if w not in word2id:
                word2id[w] = len(vocab)
                vocab.append(w)
            id_doc.append(word2id[w])
        D.append(np.array(id_doc, dtype=np.int32))

        for x in values[1].split():
            if x not in author2id:
                author2id[x] = len(authors)
                authors.append(x)
            id_author.append(author2id[x])
        A.append(np.array(id_author, dtype=np.int32))
        pbar.update(n=1)

    vocab = np.array(vocab, dtype=np.unicode_)
    authors = np.array(authors, dtype=np.unicode_)

    return D, A, vocab, authors, word2id, author2id


def assign_random_topic(D, K):
    logging.info("Randomly initializing topic assignments ...")
    Z = []
    for d in range(len(D)):
        Z.append(np.random.randint(K, size=len(D[d])))
    return Z


def assign_random_author(D, A):
    logging.info("Randomly initializing author assignments ...")
    X = []
    for d in range(len(D)):
        X.append(np.random.randint(len(A[d]), size=len(D[d])))
    return X


def save(vocab, authors, Z, n_kw, n_dk, n_ak, n_k, n_d, n_a, alpha, beta, prefix, output_dir='.', topn=20):
    logging.info("Writing output from the last sample ...")
    logging.info("Number of top topical words: {:d}".format(topn))

    output_dir = Path(output_dir)
    save_topic(vocab, n_kw, n_k, beta, topn, prefix, output_dir)
    save_z(Z, prefix, output_dir)
    save_theta(n_dk, n_d, alpha, prefix, output_dir)
    # save_phi(n_kw, n_k, beta, prefix, output_dir)
    # save_informative_word(vocab, n_kw, n_k, beta, topn, prefix, output_dir)
    save_author_topic(authors, n_ak, n_a, alpha, prefix, output_dir)


def topics(vocab, n_kw, topn):
    topics = []
    for k in range(len(n_kw)):
        topn_indices = matutils.argsort(n_kw[k], topn=topn, reverse=True)
        topics.append([vocab[w] for w in topn_indices])
    return topics


def save_topic(vocab, n_kw, n_k, beta, topn, prefix, output_dir):
    logging.info("Writing topics ...")
    output_path = output_dir / (prefix + ".topic")
    K = len(n_kw)
    V = n_kw.shape[1]
    with open(output_path.as_posix(), "w") as fo:
        pbar, pbar_text = get_progress_bar(K)
        for k in range(K):
            topn_indices = matutils.argsort(n_kw[k], topn=topn, reverse=True)
            print(" ".join(["{:s}*{:.8f}".format(vocab[w], ((n_kw[k, w] + beta) /
                                                        (n_k[k] + V * beta))) for w in topn_indices]), file=fo)
            if hasattr(pbar, 'progress'):
                pbar.progress(100*(k+1)//K)
                pbar_text.text(f'Saving topics... {k+1}')
            else:
                pbar.update(n=1)


def save_theta(n_dk, n_d, alpha, prefix, output_dir):
    logging.info("Writing θ ...")
    output_path = output_dir / (prefix + ".theta")
    N = len(n_dk)
    K = n_dk.shape[1]
    with open(output_path.as_posix(), "w") as fo:
        if notebook:
            pbar = tqdm_notebook(range(N))
        else:
            pbar = tqdm(range(N))
        for d in pbar:
            print(" ".join(["{:.8f}".format((n_dk[d, k] + alpha)/(n_d[d] + K * alpha))
                            for k in range(K)]), file=fo)


def save_author_topic(authors, n_ak, n_a, alpha, prefix, output_dir):
    logging.info("Writing author topic distributions ...")
    output_path = output_dir / (prefix + ".author")
    S = len(n_ak)
    K = n_ak.shape[1]
    with open(output_path.as_posix(), "w") as fo:
        if notebook:
            pbar = tqdm_notebook(range(S))
        else:
            pbar = tqdm(range(S))
        for a in pbar:
            author = authors[a]
            print(author + "," + " ".join(["{:.8f}".format((n_ak[a, k] + alpha)/(n_a[a] + K * alpha))
                                           for k in range(K)]), file=fo)


def save_z(Z, prefix, output_dir):
    logging.info("Writing z ...")
    output_path = output_dir / (prefix + ".z")
    with open(output_path.as_posix(), "w") as fo:
        if notebook:
            pbar = tqdm_notebook(range(len(Z)))
        else:
            pbar = tqdm(range(len(Z)))
        for d in pbar:
            print(" ".join(Z[d].astype(np.unicode_)), file=fo)


def save_phi(n_kw, n_k, beta, prefix, output_dir):
    logging.info("Writing Φ ...")
    output_path = output_dir / (prefix + ".phi")
    K = len(n_kw)
    V = n_kw.shape[1]
    with open(output_path.as_posix(), "w") as fo:
        if notebook:
            pbar = tqdm_notebook(range(K))
        else:
            pbar = tqdm(range(K))
        for k in pbar:
            print(" ".join(["{:.8f}".format((n_kw[k, w] + beta)/(n_k[k] + V * beta))
                            for w in range(V)]), file=fo)


def save_informative_word(vocab, n_kw, n_k, beta, topn, prefix, output_dir):
    logging.info("Writing important words ...")
    output_path = output_dir / (prefix + ".jlh")
    K = len(n_kw)
    V = n_kw.shape[1]
    n_w = {}
    topics = []
    with open(output_path.as_posix(), "w") as fo:
        for w in range(V):
            n_w[w] = n_kw[:, w].sum()

        pbar, pbar_text = get_progress_bar(K)

        jlh_scores = np.zeros((K, V), dtype=np.float32)
        for k in range(K):
            for w in range(V):
                glo = (n_kw[k, w] + beta)/(n_w[w] + V * beta)
                loc = (n_kw[k, w] + beta)/(n_k[k] + V * beta)
                jlh_scores[k, w] = (glo-loc) * (glo/loc)
            topn_informative_words = matutils.argsort(jlh_scores[k], topn=topn, reverse=True)
            print(" ".join(["{:s}*{:.8f}".format(vocab[w], jlh_scores[k, w])
                for w in topn_informative_words]), file=fo)
            #topics.append((k, [(vocab[w], float(jlh_scores[k, w])) for w in topn_informative_words]))
            if hasattr(pbar, 'progress'):
                pbar.progress(100*(k+1)//K)
                pbar_text.text(f'Calculating JLH... {k+1}')
            else:
                pbar.update(n=1)
        #print(json.dumps(topics), file=fo)


def get_progress_bar(n_iter):
    if from_streamlit:
        import streamlit as st
        pbar_text = st.empty()
        pbar = st.progress(0)
    elif notebook:
        pbar_text = None
        pbar = tqdm_notebook(total=n_iter)
    else:
        pbar_text = None
        pbar = tqdm(total=n_iter)
    return pbar, pbar_text
